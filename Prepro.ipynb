{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c00c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# from kan import KAN\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler,MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, balanced_accuracy_score, average_precision_score,\n",
    "    matthews_corrcoef, cohen_kappa_score, brier_score_loss, roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# ============================================================\n",
    "# 1. SEEDING (REPRODUCIBILITY)\n",
    "# ============================================================\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d58b058",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36450df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_skewed(df, except_cols):\n",
    "    skew = df.drop(columns=except_cols).skew()\n",
    "    skew_cols = skew[abs(skew) > 0.75].index.tolist()\n",
    "\n",
    "    for col in skew_cols:\n",
    "        df[col] = np.log1p(df[col])\n",
    "    return df\n",
    "\n",
    "def apply_imputation(df):\n",
    "    imputer = IterativeImputer(random_state=42)\n",
    "    arr = imputer.fit_transform(df)\n",
    "    return pd.DataFrame(arr, columns=df.columns)\n",
    "\n",
    "\n",
    "def balance_data(df, label_col=\"Sickness\"):\n",
    "    majority = df[df[label_col] == 1]\n",
    "    minority = df[df[label_col] == 0]\n",
    "\n",
    "    minority_up = resample(minority,\n",
    "                           replace=True,\n",
    "                           n_samples=len(majority),\n",
    "                           random_state=42)\n",
    "\n",
    "    df_bal = pd.concat([majority, minority_up], axis=0)\n",
    "    return df_bal.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8daabd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"IndianLiverPatientDataset(ILPD).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c215b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()\n",
    "\n",
    "df_cleaned['Gender'] = df_cleaned['Gender'].map({'Male': 0, 'Female': 1})\n",
    "df_cleaned['Sickness'] = df_cleaned['Sickness'].replace(2, 0)\n",
    "\n",
    "df_cleaned['A/G'] = df_cleaned['A/G'].fillna(df_cleaned['A/G'].mean())\n",
    "\n",
    "\n",
    "# Transform skewed excluding label + gender\n",
    "df_cleaned = log_transform_skewed(df_cleaned, [\"Sickness\", \"Gender\"])\n",
    "\n",
    "# Imputation\n",
    "df_cleaned = apply_imputation(df_cleaned)\n",
    "\n",
    "# Balance dataset\n",
    "df_cleaned = balance_data(df_cleaned, label_col=\"Sickness\")\n",
    "\n",
    "# ===============================================================\n",
    "# TRAIN–TEST SPLIT\n",
    "# ===============================================================\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df_cleaned,\n",
    "    test_size=0.2,\n",
    "    stratify=df_cleaned[\"Sickness\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_df = train.drop(\"Sickness\", axis=1)\n",
    "y_train_df = train[\"Sickness\"]\n",
    "X_test_df = test.drop(\"Sickness\", axis=1)\n",
    "y_test_df = test[\"Sickness\"]\n",
    "\n",
    "shuffle_idx = np.random.permutation(len(X_train_df))\n",
    "X_train_df = X_train_df.iloc[shuffle_idx]\n",
    "y_train_df = y_train_df.iloc[shuffle_idx]\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 2: CUT 25% FOR VALIDATION (giống Keras validation_split)\n",
    "# ===============================================================\n",
    "\n",
    "dataset_size = len(X_train_df)\n",
    "val_size = int(0.25 * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "X_train_split_df = X_train_df.iloc[:train_size]\n",
    "y_train_split_df = y_train_df.iloc[:train_size]\n",
    "\n",
    "X_val_split_df = X_train_df.iloc[train_size:]\n",
    "y_val_split_df = y_train_df.iloc[train_size:]\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 3: FIT PREPROCESSING ONLY ON TRAIN_SPLIT\n",
    "# ===============================================================\n",
    "\n",
    "# --- Standard Scaler ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_split_df)\n",
    "X_val_scaled   = scaler.transform(X_val_split_df)\n",
    "X_test_scaled  = scaler.transform(X_test_df)\n",
    "\n",
    "# --- PCA ---\n",
    "pca = PCA(n_components=7)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca   = pca.transform(X_val_scaled)\n",
    "X_test_pca  = pca.transform(X_test_scaled)\n",
    "\n",
    "# --- Factor Analysis ---\n",
    "fa = FactorAnalysis(n_components=7)\n",
    "X_train_fa = fa.fit_transform(X_train_scaled)\n",
    "X_val_fa   = fa.transform(X_val_scaled)\n",
    "X_test_fa  = fa.transform(X_test_scaled)\n",
    "\n",
    "# --- LDA ---\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train_split_df)\n",
    "X_val_lda   = lda.transform(X_val_scaled)\n",
    "X_test_lda  = lda.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 4: CONCATENATE PCA + FA + LDA\n",
    "# ===============================================================\n",
    "\n",
    "X_train_final = np.concatenate([X_train_pca, X_train_fa, X_train_lda], axis=1)\n",
    "X_val_final   = np.concatenate([X_val_pca,   X_val_fa,   X_val_lda],   axis=1)\n",
    "X_test_final  = np.concatenate([X_test_pca,  X_test_fa,  X_test_lda],  axis=1)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 5: CONVERT TO TORCH TENSORS\n",
    "# ===============================================================\n",
    "\n",
    "X_train = torch.tensor(X_train_final, dtype=torch.float32)\n",
    "X_val   = torch.tensor(X_val_final,   dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test_final,  dtype=torch.float32)\n",
    "\n",
    "# Scale to π/2 (giống code của bạn)\n",
    "X_train *= (np.pi / 2)\n",
    "X_val   *= (np.pi / 2)\n",
    "X_test  *= (np.pi / 2)\n",
    "\n",
    "y_train = torch.tensor(y_train_split_df.values, dtype=torch.float32).unsqueeze(1)\n",
    "y_val   = torch.tensor(y_val_split_df.values,   dtype=torch.float32).unsqueeze(1)\n",
    "y_test  = torch.tensor(y_test_df.values,        dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# STEP 6: DATALOADERS\n",
    "# ===============================================================\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, y_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(X_val, y_val),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test, y_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
